---
layout: post
title:  "Vectorized RL & Value Iteration"
author: "Till Zemann"
date:   2022-12-04 14:36:41 +0200
categories: jekyll update
comments: true
back_to_top_button: true
math: true
positive_reward: true
reward: 1
tags: [reinforcement learning, linear algebra]
thumbnail: "/images/rl1/linear-algebra.svg"
---

<!-- for multiple tags use a list: [hello1, hello2] -->

<!--
### Contents
* TOC
{:toc}
-->

<!--
TODO:
- add image links to References
-->
<div class="img-block" style="width: 300px;">
    <img src="/images/rl1/linear-algebra.svg"/>
</div>

### Introduction


### Value iteration

You can use the analytical solution \eqref{eq:2} to find the fixed point (=solution) $\boldsymbol{V}^\pi$ of MDPs with small state-spaces (working definition for this post: the problem is small iff the state-transition-probability matrix $\boldsymbol{P}^\pi$ with $\|S\|^2$ entries fits into main memory).

Let's say that $n = \|S\|$ for compactness in the following formulas.

$$
\begin{align*} \tag{1}\label{eq:1}
\boldsymbol{V}^{\pi} = \begin{bmatrix} V^{\pi}_{s_1} \\ V^{\pi}_{s_2} \\ \vdots \\ V^{\pi}_{s_n} \end{bmatrix} 
&=
\boldsymbol{R}^{\pi} + \gamma \boldsymbol{P}^{\pi} \boldsymbol{V}^{\pi} \\
&= \begin{bmatrix} R^{\pi}_{s_1} \\ R^{\pi}_{s_2} \\ \vdots \\ R^{\pi}_{s_n} \end{bmatrix} + \gamma 
\begin{bmatrix} 
    P^{\pi}_{s_1 s'_1} & P^{\pi}_{s_1 s'_2} & \dots\\
    \vdots & \ddots & \\
    P^{\pi}_{s_n s'_1} &        & P^{\pi}_{s_n s'_n}
\end{bmatrix}
\begin{bmatrix} V^{\pi}_{s_1} \\ V^{\pi}_{s_2} \\ \vdots \\ V^{\pi}_{s_n} \end{bmatrix}
&=
\begin{bmatrix} 
	R^{\pi}_{s_1} + \gamma \sum_{s'} P^{\pi}_{s_1 s'} V^{\pi}_{s'} \\
    \vdots \\
    R^{\pi}_{s_n} + \gamma \sum_{s'} P^{\pi}_{s_n s'} V^{\pi}_{s'} \\
\end{bmatrix}
\end{align*}
$$

The analytic solution for the fixed point (or equilibrium) $\boldsymbol{V}^{\pi}$ is:

$$
\begin{equation} \tag{2}\label{eq:2}
\boldsymbol{V}^{\pi} = (\boldsymbol{I} - \gamma \boldsymbol{P}^{\pi})^{-1} \boldsymbol{R}^{\pi}
\end{equation}
$$

If $\|S\|^2$ is too large to fit in main memory (this will be the case for the most MDPs, often times we deal with huge state spaces), but we at least know $P^a_{ss'}$ and $R^a_{ss'}$, we could use value iteration:

1. Initialize $\boldsymbol{V}^{\pi}$ randomly.
2. Do until convergence:
	3. $\boldsymbol{V}^{\pi} \leftarrow \boldsymbol{R}^{\pi} + \gamma \boldsymbol{P}^{\pi} \boldsymbol{V}^{\pi}$.

If we repeat step 3 infinitely often (=in the limit), we will get the true $V^{\pi}$.


Consider the following MDP with 4 states. The transition probabilities $P^a_{ss'} = P(S_{t+1}=s'\|s_t,a_t)$ are written on the edges.

<svg width="800" height="250" version="1.1" xmlns="http://www.w3.org/2000/svg">
	<ellipse stroke="black" stroke-width="1" fill="none" cx="292.5" cy="52.5" rx="30" ry="30"/>
	<text x="283.5" y="58.5" font-family="Times New Roman" font-size="20">s&#8321;</text>
	<ellipse stroke="black" stroke-width="1" fill="none" cx="292.5" cy="185.5" rx="30" ry="30"/>
	<text x="283.5" y="191.5" font-family="Times New Roman" font-size="20">s&#8322;</text>
	<ellipse stroke="black" stroke-width="1" fill="none" cx="457.5" cy="52.5" rx="30" ry="30"/>
	<text x="448.5" y="58.5" font-family="Times New Roman" font-size="20">s&#8323;</text>
	<ellipse stroke="black" stroke-width="1" fill="none" cx="457.5" cy="185.5" rx="30" ry="30"/>
	<text x="448.5" y="191.5" font-family="Times New Roman" font-size="20">s&#8324;</text>
	<polygon stroke="black" stroke-width="1" points="315.857,166.673 434.143,71.327"/>
	<polygon fill="black" stroke-width="1" points="434.143,71.327 424.777,72.455 431.052,80.24"/>
	<text x="380.5" y="139.5" font-family="Times New Roman" font-size="20">0.4</text>
	<polygon stroke="black" stroke-width="1" points="427.5,52.5 322.5,52.5"/>
	<polygon fill="black" stroke-width="1" points="322.5,52.5 330.5,57.5 330.5,47.5"/>
	<text x="359.5" y="43.5" font-family="Times New Roman" font-size="20">0.5</text>
	<path stroke="black" stroke-width="1" fill="none" d="M 268.188,202.875 A 22.5,22.5 0 1 1 263.92,176.771"/>
	<text x="186.5" y="204.5" font-family="Times New Roman" font-size="20">0.6</text>
	<polygon fill="black" stroke-width="1" points="263.92,176.771 259.022,168.709 254.526,177.641"/>
	<polygon stroke="black" stroke-width="1" points="457.5,82.5 457.5,155.5"/>
	<polygon fill="black" stroke-width="1" points="457.5,155.5 462.5,147.5 452.5,147.5"/>
	<text x="420.5" y="125.5" font-family="Times New Roman" font-size="20">0.5</text>
	<path stroke="black" stroke-width="1" fill="none" d="M 484.297,172.275 A 22.5,22.5 0 1 1 484.297,198.725"/>
	<text x="530.5" y="191.5" font-family="Times New Roman" font-size="20">1</text>
	<polygon fill="black" stroke-width="1" points="484.297,198.725 487.83,207.473 493.708,199.382"/>
	<polygon stroke="black" stroke-width="1" points="292.5,82.5 292.5,155.5"/>
	<polygon fill="black" stroke-width="1" points="292.5,155.5 297.5,147.5 287.5,147.5"/>
	<text x="274.5" y="125.5" font-family="Times New Roman" font-size="20">1</text>
</svg>

(...)

### Optimal policy

A policy $\pi$ is better than policy $\pi'$ if $V^\pi(s) > V^{\pi'}(s)$.

For every MDP, there exists at least one optimal policy $\pi\*$ (but could be multiple $\pi\*$ that are equally good) that is better or equal to all other policies. 
All optimal policies share the same value function:

$$
V^{*}(s) = V^{\pi*}(s) = \max_\pi V^{\pi}(s) = \max_a \left[ R_s^a + \gamma \sum_{s'} \left[ P^a_{ss'} V^{*}(s') \right] \right]
$$

- bis 1:12 geschaut

<!-- In-Text Citing -->
<!-- 

Referencing equations:
$$
\begin{equation} \tag{1}\label{eq:1}
x=y
\end{equation}
$$
I reference equation \eqref{eq:1}


You can...
- use bullet points
1. use
2. ordered
3. lists

-- Math --
$\hat{s} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu)^2$ 

-- Images --
<div class="img-block" style="width: 800px;">
    <img src="/images/lofi_art.png"/>
    <span><strong>Fig 1.1.</strong> Agent and Environment interactions</span>
</div>

-- Links --
[(k-fold) Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)

```c
for(int i=0; i<comm_sz; i++){
	print("%d\n", i);
}
```

<div class="output">
result: 42
</div>

{% highlight python %}
@jit
def f(x)
    print("hi")
# does cool stuff
{% endhighlight %}

-- Highlights --
AAABC `ASDF` __some bold text__

-- Colors --
The <strong style="color: #1E72E7">joint distribution</strong> of $X$ and $Y$ is written as $P(X, Y)$.
The <strong style="color: #ED412D">marginal distribution</strong> on the other hand can be written out as a table.
-->

### References

1. Linear-algebra thumbnail from [w3resource.com][linear-algebra-img].

<!-- Ressources -->
[RESSOURCE]: LINK
[linear-algebra-img]: https://www.w3resource.com/python-exercises/numpy/linear-algebra/index.php

<!-- Optional Comment Section-->
{% if page.comments %}
<p class="vspace"></p>
<a class="commentlink" role="button" href="/comments/">Post a comment.</a> <!-- role="button"  -->
{% endif %}

<!-- Optional Back to Top Button -->
{% if page.back_to_top_button %}
<script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 40,
  backgroundColor: 'rgb(255, 255, 255, 0.7)', /* 30,144,255, 0.7 */
  textColor: '#4a4946'
})</script>
{% endif %}