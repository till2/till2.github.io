---
layout: post
title:  "Policy Evaluation and Value Iteration"
author: "Till Zemann"
date:   2022-12-04 14:36:41 +0200
categories: jekyll update
comments: true
back_to_top_button: true
math: true
positive_reward: true
reward: 1
tags: [reinforcement learning, linear algebra]
thumbnail: "/images/rl1/linear-algebra.svg"
---

<!-- for multiple tags use a list: [hello1, hello2] -->

<!--
### Contents
* TOC
{:toc}
-->

<!--
TODO:
- add image links to References
-->
<div class="img-block" style="width: 300px;">
    <img src="/images/rl1/linear-algebra.svg"/>
</div>

### Introduction

If we have a finite set of states, we can write the state values as a vector, where the entry at position $i$ corresponds to the state value $V^{\pi}_{s_i}$ of state $s_i$:

($n = \|S\|$ so that the formulas are easier to read).
To see what's going on, let's write out the vectorized bellman equation for the state-values of policy $\pi$ and open them up:

$$
\begin{align*} \tag{1}\label{eq:1}
\boldsymbol{V}^{\pi} = \begin{bmatrix} V^{\pi}_{s_1} \\ V^{\pi}_{s_2} \\ \vdots \\ V^{\pi}_{s_n} \end{bmatrix} 
&=
\boldsymbol{R}^{\pi} + \gamma \boldsymbol{P}^{\pi} \boldsymbol{V}^{\pi} \\
&= \begin{bmatrix} R^{\pi}_{s_1} \\ R^{\pi}_{s_2} \\ \vdots \\ R^{\pi}_{s_n} \end{bmatrix} + \gamma 
\begin{bmatrix} 
    P^{\pi}_{s_1 s'_1} & P^{\pi}_{s_1 s'_2} & \dots\\
    \vdots & \ddots & \\
    P^{\pi}_{s_n s'_1} &        & P^{\pi}_{s_n s'_n}
\end{bmatrix}
\begin{bmatrix} V^{\pi}_{s_1} \\ V^{\pi}_{s_2} \\ \vdots \\ V^{\pi}_{s_n} \end{bmatrix}
&=
\begin{bmatrix} 
	R^{\pi}_{s_1} + \gamma \sum_{s'} P^{\pi}_{s_1 s'} V^{\pi}_{s'} \\
    \vdots \\
    R^{\pi}_{s_n} + \gamma \sum_{s'} P^{\pi}_{s_n s'} V^{\pi}_{s'} \\
\end{bmatrix}
\end{align*}
$$

### Analytic solution

<!-- Chat-GPT -->
To solve for $\boldsymbol{V}^{\pi}$ in equation \ref{eq:1}, we can first subtract $\gamma \boldsymbol{P}^{\pi} \boldsymbol{V}^{\pi}$ from both sides of the equation to get both $\boldsymbol{V}^{\pi}$ on one side:

$$\boldsymbol{V}^{\pi} - \gamma \boldsymbol{P}^{\pi} \boldsymbol{V}^{\pi} = \boldsymbol{R}^{\pi}$$

Next, we can factor out $\boldsymbol{V}^{\pi}$ from the left-hand side to get

$$(\boldsymbol{I} - \gamma \boldsymbol{P}^{\pi}) \boldsymbol{V}^{\pi} = \boldsymbol{R}^{\pi}$$

<em>Note that for the factorization of $\boldsymbol{V}^{\pi}$, we don't get $\boldsymbol{V}^{\pi} = 1 * \boldsymbol{V}^{\pi}$, but instead we need to use the identity matrix: $\boldsymbol{V}^{\pi} = \boldsymbol{I} \boldsymbol{V}^{\pi}$. </em>

Finally, we can multiply with the inverse matrix of $(1 - \gamma \boldsymbol{P}^{\pi})$ to obtain the analytic fixed point (or equlibrium) of the true state values $\boldsymbol{V}^{\pi}$ for policy $\pi$:

$$
\begin{equation} \tag{2}\label{eq:2}
\boldsymbol{V}^{\pi} = (\boldsymbol{I} - \gamma \boldsymbol{P}^{\pi})^{-1} \boldsymbol{R}^{\pi}
\end{equation}
$$


### Policy evaluation


__Policy evaluation algorithm:__
<hr>
__Input:__ policy $\pi$ <br>
__Output:__ approximated $V^{\pi}$

0. Initialize $V(s), V'(s)$ arbitrarily (e.g. to 0) 

1. While True:<br>
	1. &nbsp; $\Delta \leftarrow 0$ &nbsp; // maximum difference of a state-value <br>
	2. &nbsp; For each $s \in S$:<br>
		1. &nbsp; $V'(s) \leftarrow \sum_{a} \pi(s,a) \sum_{s'} P_{s,s'}^{a}[R_{s,s'}^{a} + \gamma V(s')]$<br>
		2. &nbsp; $\Delta \leftarrow$ max $(\Delta, \vert V'(s)-V(s)\vert)$<br>
		3. &nbsp; $V(s) \leftarrow V'(s)$
	3. If $\Delta < \epsilon$:
		1. return approximated $V^{\pi}$



### Value iteration

You can use the analytic solution \eqref{eq:2} to find the fixed point of $\boldsymbol{V}^\pi$ for MDPs with small state-spaces (working definition for this post: the problem is small iff the state-transition-probability matrix $\boldsymbol{P}^\pi$ with $\|S\|^2$ entries fits into main memory). If $\|S\|^2$ is too large though, so that it doesn't fit into main memory (this will be the case for many MDPs that we encounter, and often times we deal with huge state spaces), we can find an approximate solution by performing value iteration.
For the value iteration algorithm to work, the following criteria must be met:
- We need the transition probabilities $P^a_{ss'} = P(s'\|s,a)$.
- The probabilitiy distribution $P(r\|s,a,s')$ for the random variable $R^a_{ss'}$ must also be available to us.


__Value iteration algorithm:__
<hr>
__Input:__ MDP with a finite number of states and actions <br>
__Output:__ approximated $V^{\*}$

0. Initialize $V(s), V'(s)$ arbitrarily (e.g. to 0) 

1. While True:<br>
	1. &nbsp; $\Delta \leftarrow 0$ &nbsp; // maximum difference of a state-value <br>
	2. &nbsp; For each $s \in S$:<br>
		1. &nbsp; $V'(s) \leftarrow \max_{a} \sum_{s'}\sum_{r} P(s',r \| s,a) [r + \gamma V(s')]$<br>
		2. &nbsp; $\Delta \leftarrow$ max $(\Delta, \vert V'(s)-V(s)\vert)$<br>
		3. &nbsp; $V(s) \leftarrow V'(s)$
	3. If $\Delta < \epsilon$:
		1. return approximated $V^{\*}$.

If we repeat this infinitely often, then the state values will converge to the true value function $V^{\pi}$:

$$
\lim_{k \to \infty} \boldsymbol{V}^{k} = \boldsymbol{V}^{\pi}
$$

where $\boldsymbol{V}^{k}$ are the state-values in the $k$-th iteration.


To illustrate this, we can use this example MDP with a 4x4 grid as the state-space and two terminal states (upper left and bottom right cells). The action-space is $A=$ { $\text{left, right, up, down}$ } and each timestep that the agent is not in a terminal state, it receives a reward of -1. 

<!-- Value iteration image-->
<div class="img-block" style="width: 800px;">
    <img src="/images/rl1/value_iteration.png"/>
</div>
<center><a href="#references">
[3]: Value iteration visualized
</a></center>

__Deriving an optimal policy:__

From the optimal state-value function $V^{\*}$, it's relatively simple to derive an optimal policy (there could always be multiple policies that are equally good).

$$
\pi^{*}(s) = arg\max_{a} \sum_{s'}\sum_{r} P(s',r | s,a) [r + \gamma V(s')]
$$

An optimal policy for our gridworld-MDP would be:
<!-- Optimal policy image-->
<div class="img-block" style="width: 300px;">
    <img src="/images/rl1/optimal_policy.png"/>
</div>
<center><a href="#references">
[3] An optimal policy
</a></center>

<!--
Consider the following Markov reward process with 4 states. The transition probabilities $P^a_{ss'} = P(S_{t+1}=s'\|s_t,a_t)$ are written on the edges.

<svg width="800" height="250" version="1.1" xmlns="http://www.w3.org/2000/svg">
	<ellipse stroke="black" stroke-width="1" fill="none" cx="292.5" cy="52.5" rx="30" ry="30"/>
	<text x="283.5" y="58.5" font-family="Times New Roman" font-size="20">s&#8321;</text>
	<ellipse stroke="black" stroke-width="1" fill="none" cx="292.5" cy="185.5" rx="30" ry="30"/>
	<text x="283.5" y="191.5" font-family="Times New Roman" font-size="20">s&#8322;</text>
	<ellipse stroke="black" stroke-width="1" fill="none" cx="457.5" cy="52.5" rx="30" ry="30"/>
	<text x="448.5" y="58.5" font-family="Times New Roman" font-size="20">s&#8323;</text>
	<ellipse stroke="black" stroke-width="1" fill="none" cx="457.5" cy="185.5" rx="30" ry="30"/>
	<text x="448.5" y="191.5" font-family="Times New Roman" font-size="20">s&#8324;</text>
	<polygon stroke="black" stroke-width="1" points="315.857,166.673 434.143,71.327"/>
	<polygon fill="black" stroke-width="1" points="434.143,71.327 424.777,72.455 431.052,80.24"/>
	<text x="380.5" y="139.5" font-family="Times New Roman" font-size="20">0.4</text>
	<polygon stroke="black" stroke-width="1" points="427.5,52.5 322.5,52.5"/>
	<polygon fill="black" stroke-width="1" points="322.5,52.5 330.5,57.5 330.5,47.5"/>
	<text x="359.5" y="43.5" font-family="Times New Roman" font-size="20">0.5</text>
	<path stroke="black" stroke-width="1" fill="none" d="M 268.188,202.875 A 22.5,22.5 0 1 1 263.92,176.771"/>
	<text x="186.5" y="204.5" font-family="Times New Roman" font-size="20">0.6</text>
	<polygon fill="black" stroke-width="1" points="263.92,176.771 259.022,168.709 254.526,177.641"/>
	<polygon stroke="black" stroke-width="1" points="457.5,82.5 457.5,155.5"/>
	<polygon fill="black" stroke-width="1" points="457.5,155.5 462.5,147.5 452.5,147.5"/>
	<text x="420.5" y="125.5" font-family="Times New Roman" font-size="20">0.5</text>
	<path stroke="black" stroke-width="1" fill="none" d="M 484.297,172.275 A 22.5,22.5 0 1 1 484.297,198.725"/>
	<text x="530.5" y="191.5" font-family="Times New Roman" font-size="20">1</text>
	<polygon fill="black" stroke-width="1" points="484.297,198.725 487.83,207.473 493.708,199.382"/>
	<polygon stroke="black" stroke-width="1" points="292.5,82.5 292.5,155.5"/>
	<polygon fill="black" stroke-width="1" points="292.5,155.5 297.5,147.5 287.5,147.5"/>
	<text x="274.5" y="125.5" font-family="Times New Roman" font-size="20">1</text>
</svg>

(...)

### Optimal policy

A policy $\pi$ is better than policy $\pi'$ if $V^\pi(s) > V^{\pi'}(s)$.

For every MDP, there exists at least one optimal policy $\pi\*$ (but could be multiple $\pi\*$ that are equally good) that is better or equal to all other policies. 
All optimal policies share the same value function:

$$
V^{*}(s) = V^{\pi*}(s) = \max_\pi V^{\pi}(s) = \max_a \left[ R_s^a + \gamma \sum_{s'} \left[ P^a_{ss'} V^{*}(s') \right] \right]
$$
-->






<!-- In-Text Citing -->
<!-- 

Referencing equations:
$$
\begin{equation} \tag{1}\label{eq:1}
x=y
\end{equation}
$$
I reference equation \eqref{eq:1}


You can...
- use bullet points
1. use
2. ordered
3. lists

-- Math --
$\hat{s} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu)^2$ 

-- Images --
<div class="img-block" style="width: 800px;">
    <img src="/images/lofi_art.png"/>
    <span><strong>Fig 1.1.</strong> Agent and Environment interactions</span>
</div>

-- Links --
[(k-fold) Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)

```c
for(int i=0; i<comm_sz; i++){
	print("%d\n", i);
}
```

<div class="output">
result: 42
</div>

{% highlight python %}
@jit
def f(x)
    print("hi")
# does cool stuff
{% endhighlight %}

-- Highlights --
AAABC `ASDF` __some bold text__

-- Colors --
The <strong style="color: #1E72E7">joint distribution</strong> of $X$ and $Y$ is written as $P(X, Y)$.
The <strong style="color: #ED412D">marginal distribution</strong> on the other hand can be written out as a table.
-->

- Note: You can do the book exercises and try to get a solution from Prof. Sutton [here](http://incompleteideas.net/book/solutions-1st.html).

### References

1. Linear-algebra thumbnail from [w3resource.com][linear-algebra-img].
2. [Sutton & Barto: Reinforcement Learning][sab]
3. Lemcke, J. Intelligent Data Analysis 2: Exercise-Notebook 6 (Rl)

<!-- Ressources -->
[RESSOURCE]: LINK
[linear-algebra-img]: https://www.w3resource.com/python-exercises/numpy/linear-algebra/index.php
[sab]: http://incompleteideas.net/book/the-book-2nd.html

<!-- Optional Comment Section-->
{% if page.comments %}
<p class="vspace"></p>
<a class="commentlink" role="button" href="/comments/">Share your thoughts.</a> <!-- role="button"  -->
{% endif %}

<!-- Optional Back to Top Button -->
{% if page.back_to_top_button %}
<script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 40,
  backgroundColor: 'rgb(255, 255, 255, 0.7)', /* 30,144,255, 0.7 */
  textColor: '#4a4946'
})</script>
{% endif %}