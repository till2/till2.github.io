---
layout: post
title:  "Parallel Programming: MPI"
author: "Till Zemann"
date:   2022-11-16 14:36:41 +0200
categories: jekyll update
comments: false
math: true

---

<!--
### Contents
* TOC
{:toc}
-->

<div class="img-block" style="width: 120px; float:right; margin:45px;">
    <img src="/images/mpi/mpi-logo.jpeg" style="box-shadow:none;"/>
</div>

### Introduction

The Message Passing Interface (MPI) is a `C` and `Fortran` library, build for running multiple processes in a distributed-memory fashion (using communication). It is flexible enough to also work with shared memory, if needed.
It offers lots of different functions, but you can already get programs working with only a minimal subset of about 6 commands.


### Structure of a typical MPI program

The following structure applies to all programs:
- include the MPI header file

In the `int main(int argc, char** argv)` loop:
- declare variables
- MPI_Init(&argc, &argv);
- compute, communicate, etc.
- MPI_Finalize();


### Program execution

Running the program first requires building it via 

```sh
mpicc program.c -o program.out
```

After that, running is as easy as just calling 

```sh
mpirun -np 2 ./program.out
```

where `-np` specifies the number of processes.


### MPI constants

MPI Constants are usually integers and always capitalized (`MPI_COMM_WORLD`, `MPI_INT`, $\dots$).

### MPI return values


```c
int main(){
	printf("hi\n");
}
```

<!-- In-Text Citing -->
<!-- 
You can...
- use bullet points
1. use
2. ordered
3. lists

-- Math --
$\hat{s} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu)^2$ 

-- Images --
<div class="img-block" style="width: 800px;">
    <img src="/images/lofi_art.png"/>
    <span><strong>Fig 1.1.</strong> Agent and Environment interactions</span>
</div>

-- Links --
[(k-fold) Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)

{% highlight python %}
@jit
def f(x)
    print("hi")
# does cool stuff
{% endhighlight %}

-- Highlights --
AAABC `ASDF` __some bold text__

-- Colors --
The <strong style="color: #1E72E7">joint distribution</strong> of $X$ and $Y$ is written as $P(X, Y)$.
The <strong style="color: #ED412D">marginal distribution</strong> on the other hand can be written out as a table.
-->


### References

1. University of Saskatchewan's CMPT851: [slides for MPI, OpenMP and more][CMPT851-slides].

<!-- Ressources -->
[RESSOURCE]: LINK
[CMPT851-slides]: https://www.cs.usask.ca/~spiteri/CMPT851/notes/


<!-- Optional Comment Section-->
{% if page.comments %}
<p class="vspace"></p>
<a class="commentlink" role="button" href="/comments/">Post a comment.</a> <!-- role="button"  -->
{% endif %}