---
layout: post
title:  "Parallel Programming: MPI"
author: "Till Zemann"
date:   2022-11-16 14:36:41 +0200
categories: jekyll update
comments: false
math: true

---

<!--
### Contents
* TOC
{:toc}
-->

<div class="img-block" style="width: 120px; float:right; margin:45px;">
    <img src="/images/mpi/mpi-logo.jpeg" style="box-shadow:none;"/>
</div>

### Introduction

The Message Passing Interface (MPI) is a `C` and `Fortran` library, build for running multiple processes in a distributed-memory fashion (using communication). It is flexible enough to also work with shared memory, if needed.
It offers lots of different functions, but you can already get programs working with only a minimal subset of about 6 commands.


### Structure of a MPI program

The following structure applies to all programs:
- include the MPI header file
- declare variables
- MPI_Init(&argc, &argv);
- compute, communicate, etc.
- MPI_Finalize();

Putting everything together, we get the following structure:
```c
// include the MPI header file
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv){

	// declare variables
	int my_rank, n_processes;

	// init the MPI environment
	MPI_Init(&argc, &argv);

	// compute, communicate, etc.

	// close the MPI environment
	MPI_Finalize();
	return 0;
}
```


### Program execution

Running the program first requires building it via 

```sh
mpicc program.c -o program.out
```

After that, running is as easy as just calling 

```sh
mpirun -np 2 ./program.out
```

where `-np` specifies the number of processes.


### MPI constants and handles

MPI Constants are usually integers and always capitalized (`MPI_COMM_WORLD`, `MPI_INT`, `MPI_SUCCESS`, etc.).
The constants `MPI_COMM_WORLD` and `MPI_SUCCESS` are also "handles". A handle refer to internal MPI datastructes, i.e.
when you pass `MPI_COMM_WORLD` as an argument to a subroutine, MPI uses the refered communicator that includes all processes.

### MPI return values

The following program checks the error code that is returned as an integer by the MPI_Init subroutine:

```fortran
INTEGER IERR

CALL MPI_INIT(IERR)

IF (IERR.EQ.MPI_SUCCESS) THEN
	do stuff
ENF IF 
```
### MPI handles


<!-- In-Text Citing -->
<!-- 
You can...
- use bullet points
1. use
2. ordered
3. lists

-- Math --
$\hat{s} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu)^2$ 

-- Images --
<div class="img-block" style="width: 800px;">
    <img src="/images/lofi_art.png"/>
    <span><strong>Fig 1.1.</strong> Agent and Environment interactions</span>
</div>

-- Links --
[(k-fold) Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)

{% highlight python %}
@jit
def f(x)
    print("hi")
# does cool stuff
{% endhighlight %}

-- Highlights --
AAABC `ASDF` __some bold text__

-- Colors --
The <strong style="color: #1E72E7">joint distribution</strong> of $X$ and $Y$ is written as $P(X, Y)$.
The <strong style="color: #ED412D">marginal distribution</strong> on the other hand can be written out as a table.
-->


### References

1. University of Saskatchewan's CMPT851: [slides for MPI, OpenMP and more][CMPT851-slides].

<!-- Ressources -->
[RESSOURCE]: LINK
[CMPT851-slides]: https://www.cs.usask.ca/~spiteri/CMPT851/notes/


<!-- Optional Comment Section-->
{% if page.comments %}
<p class="vspace"></p>
<a class="commentlink" role="button" href="/comments/">Post a comment.</a> <!-- role="button"  -->
{% endif %}