---
layout: post
title:  "Trust region methods"
author: "Till Zemann"
date:   2022-12-22 16:31:41 +0200
categories: jekyll update
comments: true
back_to_top_button: true
math: true
positive_reward: true
reward: 2
tags: [reinforcement learning, ðŸŸ¢ not finished]
thumbnail: "/images/trpo-ppo/thumbnail.jpeg"
---

<!-- add the actor-critic diagram from Prof. Sutton.! -->

<div class="img-block" style="width: 300px;">
    <img src="/images/trpo-ppo/thumbnail.jpeg"/>
</div>

<!-- <em style="float:right">First draft: 2022-10-22</em><br> -->

<!--
### Contents
* TOC
{:toc}
-->

### Introduction

- Trust region methods: TRPO and PPO
- bad policy leads to bad data
- TRPO needs 2nd order derivative, hard to implement (?) -> just use PPO
- PPO has KL and Clip variants
- KL divergence approximation trick (see kl, some blogpost)
- Clip is easier to implement
- Show clip plots

### Importance Sampling

- see importance sampling post, link

### TRPO


### PPO


### TODO

- first look at [importance sampling](https://youtu.be/C3p2wI4RAi8)
- then watch Pieters lecture and take notes
- then research anything that's unclear
- implement it

<!-- In-Text Citing -->
<!-- 
You can...
- use bullet points
1. use
2. ordered
3. lists


-- Math --
$\hat{s} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu)^2$ 

-- Images --
<div class="img-block" style="width: 800px;">
    <img src="/images/lofi_art.png"/>
    <span><strong>Fig 1.1.</strong> Agent and Environment interactions</span>
</div>

-- Links --
[(k-fold) Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)

{% highlight python %}
@jit
def f(x)
    print("hi")
# does cool stuff
{% endhighlight %}

-- Highlights --
AAABC `ASDF` __some bold text__

-- Colors --
The <strong style="color: #1E72E7">joint distribution</strong> of $X$ and $Y$ is written as $P(X, Y)$.
The <strong style="color: #ED412D">marginal distribution</strong> on the other hand can be written out as a table.
-->

<!-- uncomment, when i understand more of the algorithms presented (missing DDPG, SAC, TD3, TRPO, PPO, Dyna-Q)
### Rl-Algorithms-Taxonomy in a Venn-Diagram

<div class="img-block" style="width: 700px;">
    <img src="/images/actor-critic/venn-diagram-rl-algos-detailed.png"/>
</div>

-->

### References
1. Thumbnail taken from [here][thumbnail-paper].
2. [Pieter Abbeel: L4 TRPO and PPO (Foundations of Deep RL Series) ][pieter-abbeel-trpo-ppo-lecture]


<!-- Ressources -->
[thumbnail-paper]: https://arxiv.org/pdf/2007.04309.pdf
[pieter-abbeel-trpo-ppo-lecture]: https://www.youtube.com/watch?v=KjWF8VIMGiY&list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0&index=4

<!-- Optional Comment Section-->
{% if page.comments %}
<p class="vspace"></p>
<a class="commentlink" role="button" href="/comments/">Post a comment.</a> <!-- role="button"  -->
{% endif %}

<!-- Optional Back to Top Button -->
{% if page.back_to_top_button %}
<script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 40,
  backgroundColor: 'rgb(255, 255, 255, 0.7)', /* 30,144,255, 0.7 */
  textColor: '#4a4946'
})</script>
{% endif %}
