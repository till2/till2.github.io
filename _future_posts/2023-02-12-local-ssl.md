---
layout: post
title:  "Local Self Supervised/ Contrastive Learning"
author: "Till Zemann"
date:   2023-02-12 10:31:41 +0200
categories: jekyll update
comments: true
back_to_top_button: true
math: true
tags: [research, bachelor thesis]
thumbnail: "/images/local-ssl/task-0000001882-b4b42454.jpg"
---


<div class="img-block" style="width: 500px;">
    <img src="/images/local-ssl/task-0000001882-b4b42454.jpg"/>
</div>


<!-- <em style="float:right">First draft: 2022-10-22</em><br> -->

<!--
### Contents
* TOC
{:toc}
-->

### Introduction

### Questions

What is Self Supervised Leraning (SSL)?
- Non-Contrastive: DINO, SwAV, DeepCluster
- Generative Models: CycleGAN, Pix2Pix

What is contrastive learning?
- first idea: subclass of Self-Supervised Learning

How do you get good contrastive samples?
- Hinton paper: need to be similar locally but dissimilar globally so that it learns global features (e.g. blend the images of two MNIST digits using randomly generated masks)

What is the potential of this tech? (potentially a good model of how the cortex learns)
What is a good baseline?
What are we optimizing for? (biological plausibility through local learning, performance)
What is the next step from the paper? (Try it for larget Networks, paper only used MNIST -> Scale it to ImageNet)
What is a good benchmark? (ImageNet)
What is Neuroplasticity and how to synapses get updated?
What kind of local learning should we look at? (layer-wise would be optimal, currently block-wise works with few blocks -> LeCun paper)
What kind of loss functions have worked so far? (Hinton puts the loss on the squared neuron activity = hidden units)
What else you you try to do?
Would you only do this for Fully-Connected (Dense) Layers or also Conv-Layers? (current guess: since Hintons loss is put on the neuron activities, you could easily use it for conv-layers as well)


### The Forward-Forward (F²) Algorithm

- optimize an objective function (i.e. "goodness function" = squared neuron activations) per layer
- the goodness function should be high for positive (real) data and low for negative (fake) data -> contrastive learning?


#### Motivations

- it is implausible that backpropagation is used in the brain [(Paper: Backprop and the brain)][backprop-and-the-brain]
-> theirfore the brain has another efficient learning algorithm (that is local and doesn't need to store the forward pass activations)
- goal: find a similar algorithm


#### Losses

- Absolute Error (L1) (ignores outliers)
- Squared Error (L1) (higher for outliers)
- Root Squared Error (also higher for outliers)
- Weighted Losses
- Huber loss (like MSE but can can)


#### Ideas

- first layers need to capture all the variance of the data so that 

#### Advantages 

- can be used for contrastive pretraining
- can be used when forward pass is not entirely differentiable or not known

#### Scale it

- [MNIST](http://yann.lecun.com/exdb/mnist/)
- [Cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)
- [ImageNet](https://towardsdatascience.com/downloading-and-using-the-imagenet-dataset-with-pytorch-f0908437c4be)

### Ressources

- [M. Weymar](https://www.uni-potsdam.de/de/emobio/team/weymar-mathias) zur Plausibilität fragen, ob alles passt ("Sprechzeiten nach Vereinbarung")



<!-- In-Text Citing -->
<!-- 
You can...
- use bullet points
1. use
2. ordered
3. lists


-- Math --
$\hat{s} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu)^2$ 

-- Images --
<div class="img-block" style="width: 800px;">
    <img src="/images/lofi_art.png"/>
    <span><strong>Fig 1.1.</strong> Agent and Environment interactions</span>
</div>

-- Links --
[(k-fold) Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)

{% highlight python %}
@jit
def f(x)
    print("hi")
# does cool stuff
{% endhighlight %}

-- Highlights --
AAABC `ASDF` __some bold text__

-- Colors --
The <strong style="color: #1E72E7">joint distribution</strong> of $X$ and $Y$ is written as $P(X, Y)$.
The <strong style="color: #ED412D">marginal distribution</strong> on the other hand can be written out as a table.
-->

<!-- uncomment, when i understand more of the algorithms presented (missing DDPG, SAC, TD3, TRPO, PPO, Dyna-Q)
### Rl-Algorithms-Taxonomy in a Venn-Diagram

<div class="img-block" style="width: 700px;">
    <img src="/images/actor-critic/venn-diagram-rl-algos-detailed.png"/>
</div>

-->

### References
1. [Thumbnail][thumbnail-paper]
2. [Geoffrey Hinton: The Forward-Forward Algorithm: Some Preliminary Investigations](https://www.cs.toronto.edu/~hinton/FFA13.pdf)
3. [Eye on AI: Geoff Hinton explains the Forward-Forward Algorithm](https://www.youtube.com/watch?v=NWqy_b1OvwQ)
4. [Merantix Momentum: Paper Reading Group - NeurIPS Highlights 2022](https://www.youtube.com/watch?v=z2Tc10zQSjQ)
5. [Edan Meyer: This Algorithm Could Make a GPT-4 Toaster Possible](https://youtu.be/rVzDRfO2sgs)
6. [Shoaib Ahmed Siddiqui, David Krueger, Yann LeCun, Stéphane Deny: Blockwise Self-Supervised Learning at Scale](https://arxiv.org/abs/2302.01647)
7. [Timothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman & Geoffrey Hinton: Backpropagation and the brain][backprop-and-the-brain]

<!-- Ressources -->
[thumbnail-paper]: https://paperswithcode.com/task/self-supervised-learning
[backprop-and-the-brain]: https://www.nature.com/articles/s41583-020-0277-3

<!-- Optional Comment Section-->
{% if page.comments %}
<p class="vspace"></p>
<a class="commentlink" role="button" href="/comments/">Post a comment.</a> <!-- role="button"  -->
{% endif %}

<!-- Optional Back to Top Button -->
{% if page.back_to_top_button %}
<script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 40,
  backgroundColor: 'rgb(255, 255, 255, 0.7)', /* 30,144,255, 0.7 */
  textColor: '#4a4946'
})</script>
{% endif %}
